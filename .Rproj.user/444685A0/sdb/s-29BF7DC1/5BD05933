{
    "contents" : "---\ntitle: \"Исследование зависимости показателя качества предсказательной модели от размера обучающей выборки\"\nauthor: \"Морозов Глеб\"\ndate: \"20 августа 2015 г.\"\noutput: \n  html_document: \n    keep_md: yes\n---\n\n```{r global_options, include=FALSE}\nknitr::opts_chunk$set(warning=FALSE, message=FALSE)\n```\n\nДанная работа исследует влияние размера обучающей выборки на качество предсказательной модели на примере данных представленных в рамках соревнования [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer) проводимого на сайте [Kaggle](https://www.kaggle.com). В качестве инструмента проведения исследования используется язык R.\n\n### Вступление\nВ процессе работы над анализом данных встречаются ситуации, когда размер выборки доступной для исследования является препятствием. С таким примером я встретился при участии в соревновании [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer) проводимого на сайте [Kaggle](https://www.kaggle.com). В качестве объекта соревнования выбрана база изображений вручную написанных цифр - [The MNIST database of handwritten digits](http://yann.lecun.com/exdb/mnist/). Изображения были отцентрованы и приведены к одинаковому размеру. В качестве обучающей выборки предлагается выборка состоящая из 42000 таких цифр. Каждая цифра разложена в строку из 784 признаков, значение в каждом является его яркостью.\n\nДля начала, загрузим полную тренировочную выборку в R.\n\n```{r cache=TRUE, results=\"hide\"}\nlibrary(readr)\nrequire(magrittr)\nrequire(dplyr)\nrequire(caret)\ndata_train <- read_csv(\"train.csv\")\n```\n\nТеперь, для получения представления о предоставленных данных, изобразим цифры в привычном для человеческого глаза виде.\n\n```{r cache=TRUE}\ncolors<-c('white','black')\ncus_col<-colorRampPalette(colors=colors)\n\ndefault_par <- par()\npar(mfrow=c(6,6),pty='s',mar=c(1,1,1,1),xaxt='n',yaxt='n')\n\nfor(i in 1:36)\n{\n        z<-array(as.matrix(data_train)[i,-1],dim=c(28,28))\n        z<-z[,28:1] \n        image(1:28,1:28,z,main=data_train[i,1],col=cus_col(256))\n}\npar(default_par)\n```\n\nДальше можно было бы приступить к построению различных моделей, выбору параметров и т.д. Но, давайте посмотрим на данные. 42000 объектов и 784 признака. При попытке построения более комплексных моделей, таких как Random Forest или Support Vector Machine я получил ошибку о нехватке памяти, а обучение даже на небольшой части от полной выборки уже происходит далеко не минуты. Один из вариантов борьбы с этим - это использование существенно более мощной машины для вычисления, либо создание кластеров из нескольких компьтеров. Но в данной работе я решил исследовать, как влияет на качество модели использование для обучение части от всех предоставленных данных.\n\n### теория обучающей кривой.\n\nВ качестве инструмента для исследования я использую Learning Curve или обучающую кривую, которая представляет собой график, состоящий из зависимости средней ошибки модели на данных использованных для обучения и зависимости средней ошибки на тестовых данных. В теории существуют два основных варианта, которые получатся при построении данного графика.\n\n![Learning Curve](figures/plot_bias_variance_examples_4.png)\n\nПервый вариант - когда модель недообучена или имеет высокое смещение (High bias). Основной признак такой ситуации - это высокая средняя ошибка как для тренировочных данных так и для тестовых. В этом случае привлечение дополнительных данных не улучшит качество модели.\nВторой вариант - когда модель переобучена или имеет большую вариативность (High variance). Визуально можно определить по наличию большого разрыва между тестовой и тренировочной кривыми и низкой тренировочной ошибкой. Тут наоборот больше данных может привести к улучшению тестовой ошибки и, соответственно, к улучшению модели.\n\n### Обработка данных\n\nРазобъём выборку на тренировочную и тестовую в соотношении 60/40.\n\n```{r cache=TRUE}\ndata_train$label <- as.factor(data_train$label)\nset.seed(111)\nsplit <- createDataPartition(data_train$label, p = 0.6, list = FALSE)\ntrain <- slice(data_train, split)\ntest <- slice(data_train, -split)\n```\n\nЕсли посмотреть на изображения цифр, приведённые выше, то можно увидеть, что, т.к. они отцентрованы, то по краям много пространства, на котором никогда не бывает самой цифры. То есть, в данных эта особенность будет выражена в признаках, которые имеют постоянное значение для всех объектов. Во-первых, такие признаки не несут никакой информации для модели и, во-вторых, для многих моделей, за исключением основанных на деревьях, могут приводить к ошибкам при обучении. Поэтому, можно удалить эти признаки из данных.\n\n```{r cache=T}\nzero_var_col <- nearZeroVar(train, saveMetrics = T)\nsum(zero_var_col$nzv)\ntrain_nzv <- train[, !zero_var_col$nzv]\ntest_nzv <- test[, !zero_var_col$nzv]\n```\n\nТаких признаков оказалось 532 из 784. Чтобы проверить как повлияло это существенное изменение на качество моделей, проведём обучение простой CART модели (на которую не должно отрицательно влиять наличие постоянных признаков) на данных до изменения и после. В качестве оценки приведено средний процент ошибки на тестовых данных.\n\n```{r cache=TRUE}\nlibrary(rpart)\nmodel_tree <- rpart(label ~ ., data = train, method=\"class\" )\npredict_data_test <- predict(model_tree, newdata = test, type = \"class\")\nsum(test$label != predict_data_test)/nrow(test)\n```\n\n```{r cache=TRUE}\nmodel_tree_nzv <- rpart(label ~ ., data = train_nzv, method=\"class\" )\npredict_data_test_nzv <- predict(model_tree_nzv, newdata = test_nzv, type = \"class\")\nsum(test_nzv$label != predict_data_test_nzv)/nrow(test_nzv)\n```\n\nТ.к. изменения затронули сотую часть процента, то можно в дальнейшем использовать данные с удалёнными признаками\n\n```{r cache=TRUE}\ntrain <- train[, !zero_var_col$nzv]\ntest <- test[, !zero_var_col$nzv]\n```\n\n### CART\nПостроим, наконец, саму обучающую кривую. Была применена простая CART модель без изменения параметров по умолчанию. Для получения статистически значимых результатов, каждая оценка проводилась на каждом значении размера выборки пять раз.\n\n```{r cache=TRUE}\nlearn_curve_data <- data.frame(integer(),\n                               double(),\n                               double())\nfor (n in 1:5 )\n{\n        for (i in seq(1, 2000, by = 200))\n        {\n                train_learn <- train[sample(nrow(train), size = i),]\n                test_learn <- test[sample(nrow(test), size = i),]\n                model_tree_learn <- rpart(label ~ ., data = train_learn, method=\"class\" )\n                predict_train_learn <- predict(model_tree_learn, type = \"class\")\n                error_rate_train_rpart <- sum(train_learn$label != predict_train_learn)/i\n                predict_test_learn <- predict(model_tree_learn, newdata = test_learn, type = \"class\")\n                error_rate_test_rpart <- sum(test_learn$label != predict_test_learn)/i\n                learn_curve_data <- rbind(learn_curve_data, c(i, error_rate_train_rpart, error_rate_test_rpart))\n        }\n}\n\n```\n\nУсреднение проводилось при помощи модели GAM.\n\n```{r fig.width= 9, fig.height= 4, cache=TRUE}\ncolnames(learn_curve_data) <- c(\"Size\", \"Train_Error_Rate\", \"Test_Error_Rate\")\nlibrary(reshape2)\nlibrary(ggplot2)\nlearn_curve_data_long <- melt(learn_curve_data, id = \"Size\")\nggplot(data=learn_curve_data_long, aes(x=Size, y=value, colour=variable)) + \n        geom_point() + stat_smooth(method = \"gam\", formula = y ~ s(x), size = 1)\n```\n\nЧто же мы видим? \n\n- Изменение среднего процента ошибки происходит монотонно, начиная с 500 объектов в выборке.\n- Ошибка как для тренировочных, так и для тестовых данных достаточно высока.\n- Разрыв между тестовыми и тренировочными данными мал.\n- Тестовая ошибка не уменьшается.\n\nЕсли суммировать - то CART модель явно недообучена, т.е. имеет постоянное высокое смещение. Увеличение выборки для обучения не приведёт к улучшению качества предсказания на тестовых данных. Для того, чтобу улучшить результаты этой модели необходимо улучшать саму модель, например вводом дополнительных значимых признаков. \n\n\n### Random Forest\nТеперь, проведём оценку Random Forest модели. Опять же модель применялась \"как есть\", никакие параметры не изменялись. Начальный размер выборки изменён на 100, т.к. модель не может быть построена, если признаков существенно больше, чем объектов.\n\n```{r cache=TRUE}\nlibrary(randomForest)\nlearn_curve_data <- data.frame(integer(),\n                               double(),\n                               double())\nfor (n in 1:5 )\n{\n        for (i in seq(100, 5100, by = 1000))\n        {\n                train_learn <- train[sample(nrow(train), size = i),]\n                test_learn <- test[sample(nrow(test), size = i),]\n                model_learn <- randomForest(label ~ ., data = train_learn)\n                predict_train_learn <- predict(model_learn)\n                error_rate_train <- sum(train_learn$label != predict_train_learn)/i\n                predict_test_learn <- predict(model_learn, newdata = test_learn)\n                error_rate_test <- sum(test_learn$label != predict_test_learn)/i\n                learn_curve_data <- rbind(learn_curve_data, c(i, error_rate_train, error_rate_test))\n        }\n}\n```\n\n```{r fig.width= 9, fig.height= 4, cache=TRUE}\ncolnames(learn_curve_data) <- c(\"Size\", \"Train_Error_Rate\", \"Test_Error_Rate\")\nlearn_curve_data_long <- melt(learn_curve_data, id = \"Size\")\nggplot(data=learn_curve_data_long, aes(x=Size, y=value, colour=variable)) + \n        geom_point() + stat_smooth()\n```\n\nТут мы видим другую ситуацию.\n\n- Изменение среднего процента ошибки также происходит монотонно.\n- Тестовая и тренировочная ошибка малы и продолжают уменьшаться.\n- Разрыв между тестовыми и тренировочными данными мал.\n\nЯ считаю, что данный график показывает возможный третий вариант, т.е. здесь нет переобучения, т.к. нет разрыва между кривыми, но и нет явного недообучения. Я бы сказал, что при увеличенни выборки будет происходить постепенное снижение тестовой и тренировочной ошибки, пока они не достигнут ограничении внутренне свойственных модели и улучшение не прекратится. В этом случае график будет похож на недообученную. Поэтому, я думаю, что увеличение размера выборки должно привести, пусть к небольшому, но улучшению качества модели и, соответственно, имеет смысл.\n\n\n### Support Vector Machine\nПрежде чем приступить к исследованию третьей модели - Support Vector Machine, необходимо ещё раз обработать данные. Проведём их стандартизацию, т.к. это необходимо для \"сходимости\" алгоритма.\n\n```{r cache=TRUE}\nlibrary(\"e1071\")\nscale_model <- preProcess(train[, -1], method = c(\"center\", \"scale\"))\ntrain_scale <- predict(scale_model, train[, -1])\ntrain_scale <- cbind(train[, 1], train_scale)\ntest_scale <- predict(scale_model, test[, -1])\ntest_scale <- cbind(test[, 1], test_scale)\n```\n\nТеперь построим график.\n\n```{r cache=TRUE}\nlearn_curve_data <- data.frame(integer(),\n                               double(),\n                               double())\nfor (n in 1:5 )\n{\n        for (i in seq(10, 2010, by = 100))\n        {\n                train_learn <- train_scale[sample(nrow(train_scale), size = i),]\n                test_learn <- test_scale[sample(nrow(test_scale), size = i),]\n                model_learn <- svm(label ~ ., data = train_learn, kernel = \"radial\", scale = F)\n                predict_train_learn <- predict(model_learn)\n                error_rate_train <- sum(train_learn$label != predict_train_learn)/i\n                predict_test_learn <- predict(model_learn, newdata = test_learn)\n                error_rate_test <- sum(test_learn$label != predict_test_learn)/i\n                learn_curve_data <- rbind(learn_curve_data, c(i, error_rate_train, error_rate_test))\n        }\n}\n```\n\n```{r fig.width= 9, fig.height= 4, cache=TRUE}\ncolnames(learn_curve_data) <- c(\"Size\", \"Train_Error_Rate\", \"Test_Error_Rate\")\nlearn_curve_data_long <- melt(learn_curve_data, id = \"Size\")\nggplot(data=learn_curve_data_long, aes(x=Size, y=value, colour=variable)) + \n        geom_point() + stat_smooth(method = \"gam\", formula = y ~ s(x), size = 1)\n```\n\n- Тренировочная ошибка очень мала.\n- Наблюдается существенный разрыв между тестовой и тренировочной кривой, который монотонно уменьшается.\n- Тестовая ошибка достаточно мала и продолжает уменьшаться.\n\nЯ думаю, что перед нами, как раз, второй вариант из теории, т.е. модель переобучена или имеет высокую вариативность. Исходя их этого вывода, можно уверенно сказать, что увеличение размера обучающей выборки приведёт к существенному улучшению качества модели.\n\n### Выводы\n\nДанная работа показала, что обучающая кривая (Learning Curve) является хорошим инструментом в арсенале исследователя данных как для оценки используемых моделей, так и для оценки необходимости в увеличении выборки используемых данных.\n\n\n\n",
    "created" : 1440068787756.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "811413872",
    "id" : "5BD05933",
    "lastKnownWriteTime" : 1440154299,
    "path" : "G:/R lang/Kaggle_Digit_Recognizer/Digit Recognizer.Rmd",
    "project_path" : "Digit Recognizer.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "type" : "r_markdown"
}